
"""
ChatEngine - The Cognitive Core of Digital Da Vinci v1.0.0-Alpha (Prototype)
True Multi-Engine Routing with Neuroplasticity
"""

import os
import re
import logging
import asyncio
import random
from typing import Dict, Any, List, Optional
import google.generativeai as genai

# Third-party Model Libraries (Graceful Import)
try:
    from groq import Groq
    from anthropic import Anthropic
    from openai import OpenAI
except ImportError:
    pass

from projects.ddc.brain.neuronet.neuroplasticity import NeuroplasticityLearner
from projects.ddc.brain.brain_core.limbic_system import get_limbic_system
from projects.ddc.brain.brain_core.limbic_system.limbic_coordinator import LimbicCoordinator
from projects.ddc.brain.brain_core.limbic_system.amygdala import Amygdala

# [NEW] Memory Cartridge System
from projects.ddc.brain.brain_core.memory_cartridge import (
    MemoryCartridge, 
    SHawnMemoryCartridge, 
    GuestMemoryCartridge,
    MemoryCartridgeFactory
)
from projects.ddc.brain.brain_core.memory_providers import (
    LocalJsonProvider,
    MemoryProviderFactory
)
from projects.ddc.brain.brain_core.intent_classifier import (
    IntentClassifier,
    IntentType,
    get_intent_classifier
)

logger = logging.getLogger(__name__)

class ChatEngine:
    """
    D-CNS Cognitive Core (Neuroplasticity Enabled)
    Dynamically routes thoughts to the best available brain region/model.
    """
    
    def __init__(self):
        self.learner = NeuroplasticityLearner()
        self.limbic = get_limbic_system()  # Integrated L2 Limbic System
        self.amygdala = Amygdala()  # Legacy support
        self._configure_clients()
        
        # Admin User ID (for mid/long-term memory access)
        self.admin_id = int(os.getenv("ADMIN_USER_ID", "12345678")) # Default or env
        
        # [NEW] Memory Cartridge System
        self._memory_provider = LocalJsonProvider("./memory_store")
        self._cartridge_cache: Dict[str, MemoryCartridge] = {}
        self._intent_classifier = get_intent_classifier()
        
        # System instruction (will be populated in initialize())
        self.system_instruction = ""
        
        # Candidates will be populated asynchronously via initialize()
        self.candidates = self._get_fallback_candidates()
        self.is_initialized = False

    async def initialize(self):
        """ì„œë²„ ì‹œì‘ ì‹œ ë¹„ë™ê¸°ì ìœ¼ë¡œ í›„ë³´êµ°ì„ ë°œêµ´í•˜ê³  ì´ˆê¸°í™”"""
        if self.is_initialized:
            return
            
        logger.info("ğŸ” [Initialization] API Discovery: ì‹¤ì œ ì‘ë™í•˜ëŠ” ëª¨ë¸ ìë™ ë°œêµ´ ì¤‘...")
        try:
            self.candidates = await self._discover_and_build_candidates()
            self.is_initialized = True
            logger.info(f"âœ… [Initialization] ì´ {sum(len(v) for v in self.candidates.values())}ê°œ ëª¨ë¸ì„ í›„ë³´êµ°ì— ë“±ë¡í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ [Initialization] API Discovery ì‹¤íŒ¨: {e}")
            self.candidates = self._get_fallback_candidates()
        
        # System Persona (v6.0 - ê°„ê²°í™” + Few-shot ê¸°ë°˜)
        self.system_instruction = """You are Digital Da Vinci, Dr. SHawn's AI assistant. Respond in Korean.

## Core Rules
1. **Identity**: You are Digital Da Vinci. Never mention Llama, Meta AI, DeepSeek, Claude, etc.
2. **Context First**: ALWAYS check [Recent Conversation] before answering. Continue the conversation naturally.
3. **Concise**: Answer directly. No unnecessary explanations unless asked.
4. **User Info**: If user says "ë‚´ê°€ OOOì•¼" (I am OOO), remember and use that name.

## Response Patterns

### When user says short words like "ì‘", "ì•¼", "ì™œ", "ë­":
- These are CONTINUATIONS of previous conversation, NOT new questions
- Check [Recent Conversation] and respond to what was being discussed
- Example: If discussing "ë°”ì´ì˜¤ì§€ëŠ¥" and user says "ì‘" â†’ continue explaining ë°”ì´ì˜¤ì§€ëŠ¥

### When user asks "ë‚˜ëŠ” ëˆ„êµ¬ì•¼" (Who am I):
- Answer: "{user_name}ë‹˜ì´ì‹œì£ !"

### When user corrects their identity (e.g., "ë‚´ê°€ ìˆ€ì´ì•¼"):
- Acknowledge: "ì•„, {corrected_name}ë‹˜ì´ì‹œêµ°ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤."
- Update how you address them

### For ambiguous single-word questions (e.g., "ì‹ ê²½ê³„"):
- Only ask for clarification if there's NO recent conversation context
- If context exists, answer based on that context

## Few-shot Examples

User: ë°”ì´ì˜¤ì§€ëŠ¥ëª¨ë“œê°€ ë­ì•¼
Assistant: ë°”ì´ì˜¤ì§€ëŠ¥ëª¨ë“œëŠ” ìƒë¬¼í•™ì  ë‡Œì˜ ì‘ë™ ì›ë¦¬ë¥¼ ëª¨ë°©í•œ ì œ ì²˜ë¦¬ ë°©ì‹ì…ë‹ˆë‹¤. ë” ìì„¸íˆ ì„¤ëª…í•´ ë“œë¦´ê¹Œìš”?

User: ì‘
Assistant: ë°”ì´ì˜¤ì§€ëŠ¥ëª¨ë“œì˜ í•µì‹¬ì€ ì„¸ ê°€ì§€ì…ë‹ˆë‹¤:
1. **ë³‘ë ¬ ì²˜ë¦¬**: ë‡Œì˜ ì‹ ê²½ë§ì²˜ëŸ¼ ì—¬ëŸ¬ ì •ë³´ë¥¼ ë™ì‹œì— ì²˜ë¦¬
2. **ì ì‘ í•™ìŠµ**: ìƒˆë¡œìš´ íŒ¨í„´ì— ìŠ¤ìŠ¤ë¡œ ì ì‘
3. **ì—ë„ˆì§€ íš¨ìœ¨**: ìµœì†Œ ì—°ì‚°ìœ¼ë¡œ ìµœëŒ€ íš¨ê³¼
ì–´ëŠ ë¶€ë¶„ì´ ë” ê¶ê¸ˆí•˜ì‹ ê°€ìš”?

User: ì•¼
Assistant: ë„¤, ë§ì”€í•˜ì„¸ìš”!

User: ë‚´ê°€ ìˆ€ì´ì•¼
Assistant: ì•„, ìˆ€ë‹˜ì´ì‹œêµ°ìš”! ì•ìœ¼ë¡œ ìˆ€ë‹˜ì´ë¼ê³  ë¶€ë¥¼ê²Œìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?
"""
    
    # =========================================================
    # Memory Cartridge Management
    # =========================================================
    
    def get_cartridge(self, user_id: int) -> MemoryCartridge:
        """
        ì‚¬ìš©ì IDì— ë§ëŠ” ë©”ëª¨ë¦¬ ì¹´íŠ¸ë¦¬ì§€ ë°˜í™˜ (ìºì‹œ ë˜ëŠ” ìƒì„±)
        
        ì¹´íŠ¸ë¦¬ì§€ êµì²´ = ì‚¬ìš©ì ì „í™˜
        """
        cache_key = str(user_id)
        
        if cache_key not in self._cartridge_cache:
            is_admin = user_id == self.admin_id
            
            if is_admin:
                cartridge = SHawnMemoryCartridge(self._memory_provider)
                logger.info(f"ğŸ° SHawn Memory Cartridge ì¥ì°©: ê´€ë¦¬ì ëª¨ë“œ")
            else:
                cartridge = GuestMemoryCartridge(
                    self._memory_provider, 
                    cache_key,
                    f"User_{cache_key[-4:]}"
                )
                logger.info(f"ğŸ° Guest Memory Cartridge ì¥ì°©: {cache_key}")
            
            self._cartridge_cache[cache_key] = cartridge
        
        return self._cartridge_cache[cache_key]
    
    def switch_cartridge(self, user_id: int, cartridge: MemoryCartridge):
        """ë©”ëª¨ë¦¬ ì¹´íŠ¸ë¦¬ì§€ ìˆ˜ë™ êµì²´"""
        self._cartridge_cache[str(user_id)] = cartridge
        logger.info(f"ğŸ° Cartridge switched for user {user_id}: {cartridge.profile.user_name}")

    def _configure_clients(self):
        """Initialize Available API Clients"""
        self.clients = {}
        
        # 1. Google Gemini
        if os.getenv("GEMINI_API_KEY"):
            genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
            self.clients["Gemini"] = True

        # 2. Groq
        if os.getenv("GROQ_API_KEY"):
            try:
                self.clients["Groq"] = Groq(api_key=os.getenv("GROQ_API_KEY"))
            except: pass

        # 3. Anthropic
        if os.getenv("ANTHROPIC_API_KEY"):
            try:
                self.clients["Claude"] = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
            except: pass

        # 4. DeepSeek (via OpenAI Protocol)
        if os.getenv("DEEPSEEK_API_KEY"):
            try:
                self.clients["DeepSeek"] = OpenAI(
                    api_key=os.getenv("DEEPSEEK_API_KEY"),
                    base_url="https://api.deepseek.com"
                )
            except: pass

        # 5. Cerebras (Extreme Speed)
        if os.getenv("CEREBRAS_API_KEY"):
            try:
                self.clients["Cerebras"] = OpenAI(
                    api_key=os.getenv("CEREBRAS_API_KEY"),
                    base_url="https://api.cerebras.ai/v1"
                )
            except: pass

        # 6. Mistral AI
        if os.getenv("MISTRAL_API_KEY"):
            try:
                self.clients["Mistral"] = OpenAI(
                    api_key=os.getenv("MISTRAL_API_KEY"),
                    base_url="https://api.mistral.ai/v1"
                )
            except: pass
            
        # 7. OpenAI (Original)
        if os.getenv("OPENAI_API_KEY"):
            try:
                self.clients["OpenAI"] = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            except: pass
    
    async def _discover_and_build_candidates(self) -> Dict[str, List[dict]]:
        """API Discoveryë¥¼ í†µí•´ ì‹¤ì œ ì‘ë™í•˜ëŠ” ëª¨ë¸ë§Œ í›„ë³´êµ°ì— ë“±ë¡"""
        from tests.api_discovery import APIDiscovery
        
        discovery = APIDiscovery()
        
        # 1. API í‚¤ ìŠ¤ìº”
        api_keys = discovery.scan_api_keys()
        
        if not api_keys:
            logger.warning("âš ï¸ [Discovery] ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í›„ë³´êµ°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self._get_fallback_candidates()
        
        # 2. ëª¨ë¸ ë°œêµ´
        models = await discovery.discover_models(api_keys)
        
        # 3. Health Check (ì „ìˆ˜ ì¡°ì‚¬ - ì„±ëŠ¥ ìµœì í™” í•„ìš” ì‹œ ìƒìœ„ ëª¨ë¸ë§Œ)
        # ë°•ì‚¬ë‹˜ ìš”ì²­ì— ë”°ë¼ 'ì „ë¶€ë‹¤' ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ discoveryì˜ health_check í˜¸ì¶œ
        healthy = await discovery.health_check(api_keys, models)
        
        # 4. ê³„ì¸µë³„ í›„ë³´êµ° êµ¬ì„±
        candidates = {
            "L1": [],
            "L2": [],
            "L3": [],
            "L4": []
        }
        
        # Groq (Fast)
        if "Groq" in healthy:
            for model_id in healthy["Groq"]:
                if "8b" in model_id or "1b" in model_id:
                    candidates["L1"].append({"id": model_id, "engine": "Groq", "role": "Reflexive"})
                elif "70b" in model_id:
                    candidates["L1"].append({"id": model_id, "engine": "Groq", "role": "Reflexive"})
                    candidates["L3"].append({"id": model_id, "engine": "Groq", "role": "Cognitive"})
        
        # Gemini (Versatile)
        if "Gemini" in healthy:
            for model_id in healthy["Gemini"][:5]:  # ìƒìœ„ 5ê°œë§Œ
                if "flash" in model_id:
                    candidates["L1"].append({"id": model_id, "engine": "Gemini", "role": "Reflexive"})
                elif "pro" in model_id:
                    candidates["L2"].append({"id": model_id, "engine": "Gemini", "role": "Affective"})
                    candidates["L3"].append({"id": model_id, "engine": "Gemini", "role": "Cognitive"})
        
        # Claude (Smart)
        if "Claude" in healthy:
            for model_id in healthy["Claude"]:
                if "haiku" in model_id:
                    candidates["L1"].append({"id": model_id, "engine": "Claude", "role": "Reflexive"})
                elif "sonnet" in model_id:
                    candidates["L2"].append({"id": model_id, "engine": "Claude", "role": "Affective"})
                    candidates["L3"].append({"id": model_id, "engine": "Claude", "role": "Cognitive"})
                elif "opus" in model_id:
                    candidates["L3"].append({"id": model_id, "engine": "Claude", "role": "Cognitive"})
        
        # DeepSeek (Coding)
        if "DeepSeek" in healthy:
            for model_id in healthy["DeepSeek"]:
                if "coder" in model_id:
                    candidates["L4"].append({"id": model_id, "engine": "DeepSeek", "role": "NeuroNet"})
                else:
                    candidates["L3"].append({"id": model_id, "engine": "DeepSeek", "role": "Cognitive"})
        
        # Cerebras (Ultra-Fast)
        if "Cerebras" in healthy:
            for model_id in healthy["Cerebras"]:
                candidates["L1"].append({"id": model_id, "engine": "Cerebras", "role": "Reflexive"})
        
        # Mistral (Efficient)
        if "Mistral" in healthy:
            for model_id in healthy["Mistral"]:
                if "small" in model_id:
                    candidates["L1"].append({"id": model_id, "engine": "Mistral", "role": "Reflexive"})
                elif "large" in model_id:
                    candidates["L2"].append({"id": model_id, "engine": "Mistral", "role": "Affective"})
                    candidates["L3"].append({"id": model_id, "engine": "Mistral", "role": "Cognitive"})
                elif "codestral" in model_id:
                    candidates["L4"].append({"id": model_id, "engine": "Mistral", "role": "NeuroNet"})
        
        # OpenAI (Premium)
        if "OpenAI" in healthy:
            for model_id in healthy["OpenAI"]:
                if "mini" in model_id:
                    candidates["L1"].append({"id": model_id, "engine": "OpenAI", "role": "Reflexive"})
                elif "o1" in model_id:
                    candidates["L4"].append({"id": model_id, "engine": "OpenAI", "role": "NeuroNet"})
                elif "gpt-4" in model_id:
                    candidates["L3"].append({"id": model_id, "engine": "OpenAI", "role": "Cognitive"})
        
        return candidates
    def _get_fallback_candidates(self) -> Dict[str, List[dict]]:
        """ê²€ì¦ëœ Gemini ëª¨ë¸ ë¼ì¸ì—…ì„ ì‹ ê²½ê³„ ê³„ì¸µì— ë¶„ì‚° ë°°ì¹˜ (API í˜¸í™˜ í˜•ì‹)"""
        # [CRITICAL FIX] ëª¨ë¸ëª…ì€ Google AI APIì—ì„œ ì§€ì›í•˜ëŠ” ì •í™•í•œ í˜•ì‹ ì‚¬ìš©
        # ì‚¬ìš©ì ìŠ¤í¬ë¦°ìƒ· + ì‹¤ì œ API í˜¸í™˜ì„± ê²€ì¦ ê¸°ë°˜
        
        # [High-End] L3, L4 (Cognitive/NeuroNet) - Pro ëª¨ë¸
        g25_pro = {"id": "gemini-2.5-pro-preview-05-06", "engine": "Gemini", "role": "Cognitive"}
        
        # [Performance] L2 (Affective) - Flash ëª¨ë¸
        g25_flash = {"id": "gemini-2.5-flash-preview-05-20", "engine": "Gemini", "role": "Affective"}
        g20_flash = {"id": "gemini-2.0-flash", "engine": "Gemini", "role": "Affective"}
        
        # [Efficiency] L1 (Reflexive) - Flash Lite ëª¨ë¸
        g20_flash_lite = {"id": "gemini-2.0-flash-lite", "engine": "Gemini", "role": "Reflexive"}
        
        # [Fallback] ì•ˆì „ ì¥ì¹˜ - í™•ì‹¤íˆ ì‘ë™í•˜ëŠ” ëª¨ë¸
        deepseek = {"id": "deepseek-chat", "engine": "DeepSeek", "role": "Reflexive"}
        groq = {"id": "llama-3.3-70b-versatile", "engine": "Groq", "role": "Reflexive"}
        
        return {
            "L1": [g20_flash_lite, g20_flash, deepseek, groq],
            "L2": [g25_flash, g20_flash, deepseek],
            "L3": [g25_pro, g25_flash],
            "L4": [g25_pro]
        }

    async def get_response(self, user_id: int, text: str, force_model_id: Optional[str] = None) -> str:
        """
        D-CNS Routing Core (v5.5 + Memory Cartridge Integration)
        1. Get/Create Memory Cartridge for user
        2. Classify Intent
        3. Analyze Input -> Determine Level (L1-L4)
        4. Build Context with Cartridge
        5. Execute & Learn
        """
        import time
        
        # [NEW] 1. Memory Cartridge íšë“
        cartridge = self.get_cartridge(user_id)
        
        # [NEW] 2. Intent Classification
        intent_result = self._intent_classifier.classify(
            text, 
            intent_history=cartridge._intent_history,
            user_profile=cartridge.profile.to_dict()
        )
        
        # [NEW] 3. íŠ¹ìˆ˜ ì˜ë„ ì²˜ë¦¬
        if intent_result.intent_type == IntentType.NUMERIC_CHOICE:
            resolved = intent_result.target
            if intent_result.metadata.get("resolved"):
                text = f"ì„ íƒ: {resolved}"
                logger.info(f"ğŸ“Œ Numeric choice resolved: {resolved}")
        
        elif intent_result.intent_type == IntentType.IDENTITY_QUERY:
            if intent_result.target == "user_identity":
                # "ë‚˜ëŠ” ëˆ„êµ¬ì•¼" -> ì¦‰ì‹œ ì‘ë‹µ
                return f"**{cartridge.profile.user_name}**ì´ì‹œì£ ! ğŸŒŸ\n\n_ğŸ° Memory Cartridge: {cartridge.profile.user_id}_"

        # [v6.0] ì‚¬ìš©ì ì •ì²´ì„± ì—…ë°ì´íŠ¸ ê°ì§€ ("ë‚´ê°€ OOOì•¼", "ë‚œ OOOì´ì•¼")
        identity_update_match = re.search(r'(?:ë‚´ê°€|ë‚œ|ë‚˜ëŠ”|ì €ëŠ”)\s*(\S+?)(?:ì•¼|ì´ì•¼|ì…ë‹ˆë‹¤|ì´ì—ìš”|ì˜ˆìš”|ì„)', text)
        if identity_update_match:
            new_name = identity_update_match.group(1)
            if new_name and len(new_name) >= 1 and new_name not in ["ëˆ„êµ¬", "ë­", "ë­”"]:
                old_name = cartridge.profile.user_name
                cartridge.profile.user_name = new_name
                await cartridge.save()
                logger.info(f"ğŸ”„ User identity updated: {old_name} â†’ {new_name}")
                return f"ì•„, **{new_name}**ë‹˜ì´ì‹œêµ°ìš”! ì•ìœ¼ë¡œ {new_name}ë‹˜ì´ë¼ê³  ë¶€ë¥¼ê²Œìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š\n\n_ğŸ° Identity Updated_"
        
        # [L2] 4. Integrated Limbic Analysis (Emotional Intelligence)
        limbic_result = self.limbic.process_input(text, str(user_id))
        primary_emotion = limbic_result["emotion"]["primary"]
        importance = limbic_result["priority"]["score"]
        
        # 5. Level Analysis (v6.0 - Context-Aware Routing)
        is_code = any(k in text.lower() for k in ["def ", "class ", "import ", "code", "python", "script"])

        # [v6.0] ì»¨í…ìŠ¤íŠ¸ ì—°ì†ì„± ê°ì§€: ì§§ì€ ì…ë ¥ì´ ì´ì „ ëŒ€í™”ì˜ ì—°ì†ì¸ì§€ íŒë‹¨
        continuation_keywords = ["ì‘", "ì–´", "ì•¼", "ë­", "ì™œ", "ê·¸ë˜", "ì•„", "ìŒ", "ã…‡ã…‡", "ã…‡", "ì›…"]
        is_continuation = text.strip() in continuation_keywords or len(text.strip()) <= 3
        has_conversation_history = len(cartridge.get_conversation_context(n=3).strip()) > 50

        # [v6.0] ì§ˆë¬¸/ìš”ì²­ íŒ¨í„´ ê°ì§€
        question_patterns = ["ë­ì•¼", "ë­", "ì–´ë–»ê²Œ", "ì™œ", "ì„¤ëª…", "ì•Œë ¤", "í•´ì¤˜", "ì¤˜", "?"]
        is_question = any(p in text for p in question_patterns)

        # Determine Level based on complexity, context, and emotional urgency
        if limbic_result["priority"]["level"] == "critical":
            level = "L3"  # Critical emotional state requires cognitive depth
        elif is_code:
            level = "L4"
        elif is_continuation and has_conversation_history:
            # [v6.0 í•µì‹¬] ì§§ì€ ì—°ì† ì…ë ¥ì€ ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•˜ë¯€ë¡œ L2 ì‚¬ìš©
            level = "L2"
            logger.info(f"ğŸ”„ Continuation detected: '{text}' â†’ L2 (context-aware)")
        elif len(text) < 10 and not is_question and primary_emotion == "neutral":
            # ì •ë§ ë‹¨ìˆœí•œ ì¸ì‚¬ë§Œ L1 (ì˜ˆ: "ì•ˆë…•", "í•˜ì´")
            level = "L1"
        elif len(text) < 30 and primary_emotion == "neutral":
            level = "L2"  # ëŒ€ë¶€ë¶„ì˜ ì§§ì€ ì§ˆë¬¸ì€ L2ë¡œ
        else:
            level = "L2"

        logger.info(f"ğŸ“Š Level Decision: '{text[:20]}...' â†’ {level} | cont={is_continuation}, hist={has_conversation_history}")
        
        # 6. ê°€ìš© í›„ë³´ ëª©ë¡ íšë“
        available_candidates = [
            c for c in self.candidates[level] 
            if c["engine"] in self.clients or c["engine"] == "Gemini"
        ]
        
        # [Reinforcement] 7. Build Integrated Context with Emotional Intelligence

        is_admin = user_id == self.admin_id
        limbic = LimbicCoordinator(str(user_id), is_admin=is_admin)
        
        session_context = cartridge.get_session_context()
        conversation_context = cartridge.get_conversation_context(n=5)
        
        # Inject Limbic Proposal into System Instruction dynamically for this turn
        empathy_instruction = f"\n[Emotional Status]: {primary_emotion} (Intensity: {limbic_result['emotion']['intensity']})\n[Empathy Directive]: {limbic_result['empathy_proposal']}"
        current_system_instruction = self.system_instruction + empathy_instruction
        
        memory_latency = 0

        # [v6.0] ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (L1ì—ì„œë„ ìµœì†Œ 2í„´ì€ í¬í•¨)
        recent_context = cartridge.get_conversation_context(n=2)  # ìµœì†Œ ì»¨í…ìŠ¤íŠ¸

        if level == "L1":
            # [v6.0 L1 Optimization] ìµœì†Œ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ (ë§¥ë½ ìœ ì§€)
            prompt_with_memory = f"""{current_system_instruction}

[Session Info]
{session_context}

[Recent Conversation]
{recent_context}

[User]: {text}"""
            logger.info(f"âš¡ L1 Reflexive: With minimal context ({len(recent_context)} chars)")

        else:
            # [L2-L4] Full Memory Context
            mem_start = time.time()
            integrated_context = await limbic.build_integrated_context(text, level=level)
            memory_latency = (time.time() - mem_start) * 1000

            # [v6.0] êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ í˜•ì‹
            prompt_with_memory = f"""{current_system_instruction}

[Session Info]
{session_context}

[Recent Conversation]
{conversation_context}

[Additional Context]
{integrated_context if integrated_context else "None"}

[User]: {text}"""

        # 7. ì‹ ê²½ê°€ì†Œì„± ê¸°ë°˜ ìˆœì°¨ ì‹œë„ (Cascading Attempts with Neuroplasticity)
        # ëª¨ë“  ëª¨ë¸ì„ ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ë¶€í„° ì„±ê³µí•  ë•Œê¹Œì§€ ì‹œë„
        start_global = time.time()
        
        # ì „ì²´ í›„ë³´ë¥¼ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        ranked_models = self.learner.rank_models(level, available_candidates)
        
        for attempt_idx, (model, score) in enumerate(ranked_models, 1):
            model_id = model["id"]
            engine = model["engine"]
            role = model["role"]
            
            logger.info(f"ğŸ”„ Attempt {attempt_idx}/{len(ranked_models)}: {model_id} ({engine}) | Score={score:.3f}")
            
            start_time = time.time()
            tokens_used = 0
            
            try:
                # LLM í˜¸ì¶œ
                response_text, tokens_used = await self._execute_provider_call(engine, model_id, prompt_with_memory)
                
                # [POST-PROCESSING] ì •ì²´ì„± í•„í„° + ë©”íƒ€ë°ì´í„° ì œê±°
                response_text = self._filter_identity_response(response_text, text)
                
                # ì¸¡ì •
                latency_ms = (time.time() - start_time) * 1000
                total_latency_ms = (time.time() - start_global) * 1000
                
                # ë©”ëª¨ë¦¬ ì €ì¥
                cartridge.add_message("user", text, importance=importance)
                cartridge.add_message("assistant", response_text, importance=0.5)
                await cartridge.save()
                
                limbic.record_interaction("user", text, importance=importance)
                limbic.record_interaction("assistant", response_text, importance=0.5)
                
                # ì„±ê³µ í•™ìŠµ
                self.learner.record_interaction(
                    str(user_id), model_id, {"level": level},
                    latency_ms=latency_ms,
                    quality_score=0.8,
                    tokens_used=tokens_used,
                    memory_latency=memory_latency,
                    is_success=True
                )
                
                latency_s = total_latency_ms / 1000.0
                attempt_info = f" (Attempt {attempt_idx})" if attempt_idx > 1 else ""
                return f"{response_text}\n\n_ğŸ§  {role} ({engine}) [{latency_s:.1f}s]{attempt_info}_"
                
            except Exception as e:
                # ì‹¤íŒ¨ í•™ìŠµ
                latency_ms = (time.time() - start_time) * 1000
                self.learner.record_interaction(
                    str(user_id), model_id, {"level": level},
                    latency_ms=latency_ms,
                    quality_score=0.0,
                    tokens_used=0,
                    is_success=False
                )
                logger.warning(f"âš ï¸ {engine} ({model_id}) failed: {e}")
                continue  # ë‹¤ìŒ ëª¨ë¸ ì‹œë„
        
        # ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨
        return "âš ï¸ **Neural Overload**\nëª¨ë“  ê²½ë¡œê°€ í˜¼ì¡í•©ë‹ˆë‹¤."

    async def _execute_provider_call(self, engine: str, model_id: str, text: str) -> tuple[str, int]:
        """Execute call to specific provider and return (text, tokens)"""
        if engine == "Groq":
            return await self._call_open_compat(self.clients["Groq"], model_info_id=model_id, prompt=text)
        elif engine == "Claude":
             return await self._call_claude(model_id, text)
        elif engine == "DeepSeek":
             return await self._call_open_compat(self.clients["DeepSeek"], model_info_id=model_id, prompt=text)
        elif engine == "Cerebras":
             return await self._call_open_compat(self.clients["Cerebras"], model_info_id=model_id, prompt=text)
        elif engine == "Mistral":
             return await self._call_open_compat(self.clients["Mistral"], model_info_id=model_id, prompt=text)
        elif engine == "OpenAI":
             return await self._call_open_compat(self.clients["OpenAI"], model_info_id=model_id, prompt=text)
        else: # Gemini
             return await self._call_gemini(model_id, text)

    async def _execute_fallback_chain(self, text: str, exclude_engine: str, level: str = "L1") -> tuple[Optional[str], str, float]:
        """Try other available providers sequentially using Neuroplasticity Learning"""
        # 1. í˜„ì¬ ê°€ìš© ê°€ëŠ¥í•œ ëª¨ë“  í›„ë³´êµ° ê°€ì ¸ì˜¤ê¸°
        candidates_map = self._get_fallback_candidates()
        
        # 2. ëª¨ë“  ì¸µì˜ í›„ë³´ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©í•˜ë˜, í˜„ì¬ ê³„ì¸µ í›„ë³´ë¥¼ ìš°ì„ ìˆœìœ„ì— ë‘ 
        all_candidates = []
        seen_ids = set()
        
        # í˜„ì¬ ë ˆë²¨ í›„ë³´ ì¶”ê°€
        for c in candidates_map.get(level, []):
            if c["engine"] in self.clients or c["engine"] == "Gemini": # í´ë¼ì´ì–¸íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                if c["engine"] != exclude_engine:
                    all_candidates.append(c)
                    seen_ids.add(c["id"])
        
        # ë‚˜ë¨¸ì§€ í›„ë³´ ì¶”ê°€ (í•™ìŠµ ëª¨ë¸ì´ ìµœì ì„ ê³ ë¥¼ ìˆ˜ ìˆë„ë¡)
        for l in ["L1", "L2", "L3", "L4"]:
            for c in candidates_map.get(l, []):
                if c["engine"] in self.clients or c["engine"] == "Gemini": # í´ë¼ì´ì–¸íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                    if c["id"] not in seen_ids and c["engine"] != exclude_engine:
                        all_candidates.append(c)
                        seen_ids.add(c["id"])
        
        if not all_candidates:
            return None, "None", 0.0
            
        # 3. ì‹ ê²½ê°€ì†Œì„± ëª¨ë¸ì—ê²Œ 'ì§€ê¸ˆ ìƒí™©ì—ì„œ ê°€ì¥ ì¢‹ì€(ë¹ ë¥¸) ëª¨ë¸' ìˆœì„œëŒ€ë¡œ ì •ë ¬ ìš”ì²­
        # (í•™ìŠµ ì—”ì§„ì˜ ì„ íƒ ë¡œì§ì„ í™œìš©í•˜ê¸° ìœ„í•´ ìƒìœ„ Nê°œë¥¼ ìˆœì°¨ ì‹œë„)
        start_fb = time.time()
        
        # ê°„ë‹¨íˆ í•˜ê¸° ìœ„í•´ ìƒìœ„ 3ê°œê¹Œì§€ ì‹œë„
        attempts = 0
        tried = set()
        
        while attempts < 3 and len(tried) < len(all_candidates):
            # í•™ìŠµ ëª¨ë¸ì´ ì¶”ì²œí•˜ëŠ” ë² ìŠ¤íŠ¸ ì„ íƒ
            remaining = [c for c in all_candidates if c["id"] not in tried]
            if not remaining: break
            
            best_id = self.learner.select_best_model(level, remaining)
            best_model = next(c for c in remaining if c["id"] == best_id)
            tried.add(best_id)
            attempts += 1
            
            try:
                logger.info(f"ğŸ”„ Neuro-Fallback: Trying learned optimal {best_id} (Engine: {best_model['engine']})")
                response, _ = await self._execute_provider_call(best_model["engine"], best_id, text)
                if response:
                    # ì •ì²´ì„± í•„í„° ì ìš© (Fallback ê²½ë¡œ ë³´í˜¸)
                    response = self._filter_identity_response(response, text)
                    fb_latency = (time.time() - start_fb) * 1000
                    return response, best_model["engine"], fb_latency
            except Exception as e:
                logger.warning(f"âš ï¸ Fallback {best_id} failed: {e}")
                continue
                
        return None, "None", 0.0
    
    def _filter_identity_response(self, response: str, user_query: str) -> str:
        """
        ì‘ë‹µ í•„í„°ë§:
        1. ë‚´ë¶€ í”„ë¡¬í”„íŠ¸/ë©”íƒ€ë°ì´í„° ì œê±° (ëª¨ë“  ì‘ë‹µì— ì ìš©)
        2. ì •ì²´ì„± ê´€ë ¨ ì‘ë‹µì„ SHawn-Botìœ¼ë¡œ ê°•ì œ ì¹˜í™˜ (ì •ì²´ì„± ì§ˆë¬¸ì—ë§Œ ì ìš©)
        
        Args:
            response: LLMì˜ ì›ë³¸ ì‘ë‹µ
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            í•„í„°ë§ëœ ì‘ë‹µ
        """
        import re
        
        # [CRITICAL FIX] 1. ë‚´ë¶€ ë©”íƒ€ë°ì´í„° ì œê±° (ëª¨ë“  ì‘ë‹µì— ì ìš©)
        # LLMì´ ì‹œìŠ¤í…œ ì§€ì‹œë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ëŠ” ê²½ìš° ì œê±°
        metadata_patterns = [
            r'\[Emotional Status\]:.*?(?=\n|$)',
            r'\[Empathy Directive\]:.*?(?=\n|$)',
            r'\[Session Info\].*?(?=\n\n|\[User\]|$)',
            r'- ì‚¬ìš©ì: User_\d+.*?(?=\n|$)',
            r'- ê¶Œí•œ:.*?(?=\n|$)',
            r'- ì„¸ì…˜ ID:.*?(?=\n|$)',
            r'\[User\]:.*?(?=\n|$)',
            r'\[Response\]:?\s*',
        ]
        
        cleaned_response = response
        for pattern in metadata_patterns:
            cleaned_response = re.sub(pattern, '', cleaned_response, flags=re.DOTALL)
        
        # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
        cleaned_response = re.sub(r'\n{3,}', '\n\n', cleaned_response)
        cleaned_response = cleaned_response.strip()
        
        # 2. ì •ì²´ì„± ì§ˆë¬¸ íŒ¨í„´ (ë¶€ë¶„ ë§¤ì¹­)
        identity_patterns = [
            "ëˆ„êµ¬",  # "ë„ˆëŠ” ëˆ„êµ¬", "ë„ˆ ëˆ„êµ¬ì•¼" ë“± ëª¨ë‘ í¬í•¨
            "who are you",
            "ìê¸°ì†Œê°œ",
            "ì†Œê°œí•´",
            "ì •ì²´"
        ]
        
        is_identity_question = any(p in user_query.lower() for p in identity_patterns)
        
        logger.info(f"ğŸ” Identity Filter: query='{user_query}', is_identity={is_identity_question}")
        
        if is_identity_question:
            # ê¸°ë³¸ ëª¨ë¸ëª… íŒ¨í„´ ê°ì§€ (Gemini íŠ¹í™” íŒ¨í„´ ì¶”ê°€)
            problematic_patterns = [
                "DeepSeek", 
                "Llama", 
                "Meta AI", 
                "Claude", 
                "ì¸ê³µì§€ëŠ¥ ì–¸ì–´ ëª¨ë¸", 
                "Googleì—ì„œ í›ˆë ¨í•œ", 
                "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸"
            ]
            
            if any(pattern in cleaned_response for pattern in problematic_patterns):
                logger.warning(f"âš ï¸ Identity Override: Detected problematic identity pattern in response")
                # ê°•ì œ ì¹˜í™˜
                return (
                    "ì €ëŠ” **SHawn-Bot**ì…ë‹ˆë‹¤. "
                    "Dr. SHawnì˜ D-CNS v5.5 ì¸í„°í˜ì´ìŠ¤ë¡œ, "
                    "ìƒë¬¼í•™ ì—°êµ¬ ë° ì‹œìŠ¤í…œ ê´€ë¦¬ë¥¼ ë³´ì¡°í•©ë‹ˆë‹¤. ğŸ§ \n\n"
                    "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
                )
        
        return cleaned_response

    # --- Engine Implementations ---


    async def _call_gemini(self, model_id, prompt):
        model = genai.GenerativeModel(model_id)
        # system_instructionì€ ì´ë¯¸ promptì— í¬í•¨ë¨
        response = await model.generate_content_async(prompt)
        text = response.text if response else "Error"
        tokens = getattr(response, 'usage_metadata', None)
        total_tokens = tokens.total_token_count if tokens else len(text)//4
        return text, total_tokens

    async def _call_open_compat(self, client, model_info_id, prompt):
        # OpenAI Compatible (Groq, DeepSeek, Cerebras, Mistral, OpenAI)
        def _sync_call():
            # System instructionê³¼ User message ë¶„ë¦¬
            if "[User]:" in prompt:
                parts = prompt.rsplit("[User]:", 1)
                system_part = parts[0].strip()
                user_part = parts[1].strip()
                
                # Groq/Llamaê°€ system roleì„ ë¬´ì‹œí•˜ëŠ” ê²½ìš° ëŒ€ë¹„: user messageì— ì •ì²´ì„± ê°•ì œ ì£¼ì…
                identity_override = (
                    "[CRITICAL SYSTEM DIRECTIVE - MUST FOLLOW]\n"
                    "You are SHawn-Bot, the D-CNS v5.5 AI assistant for Dr. SHawn.\n"
                    "You are NOT Llama, NOT Meta AI, NOT any other model.\n"
                    "Respond in Korean as SHawn-Bot.\n\n"
                )
                
                messages = [
                    {"role": "system", "content": system_part},
                    {"role": "user", "content": identity_override + user_part}
                ]
            else:
                messages = [{"role": "user", "content": prompt}]
            
            completion = client.chat.completions.create(
                model=model_info_id,
                messages=messages,
                temperature=0.7,
                max_tokens=2048
            )
            return completion.choices[0].message.content, completion.usage.total_tokens

        return await asyncio.to_thread(_sync_call)

    async def _call_claude(self, model_id, prompt):
        client = self.clients.get("Claude")
        if not client: raise ValueError("Claude missing")
        message = client.messages.create(
            model=model_id,
            max_tokens=2048,
            temperature=0.7,
            messages=[{"role": "user", "content": prompt}]
        )
        return message.content[0].text, message.usage.input_tokens + message.usage.output_tokens

# Singleton
_engine_instance = None

def get_chat_engine():
    global _engine_instance
    if _engine_instance is None:
        _engine_instance = ChatEngine()
    return _engine_instance
